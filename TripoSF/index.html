<!DOCTYPE html>
<head>
    <meta charset="utf-8" />
    <title>SparseFlex: High-Resolution and Arbitrary-Topology 3D Shape Modeling</title>
	<link rel="icon" type="image/x-icon" href="assets/images/favicon.ico">
    <meta content="SparseFlex: High-Resolution and Arbitrary-Topology 3D Shape Modeling" name="description" />
    <meta content="summary" name="twitter:card" />
    <meta content="width=device-width, initial-scale=1" name="viewport" />
    <link href="static/css/template.css" rel="stylesheet" type="text/css" />
    <link href="static/css/my_style.css" rel="stylesheet" type="text/css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

    <!-- <script type="module">
        import * as THREE from 'https://cdn.jsdelivr.net/npm/three@0.169.0/build/three.module.js';
      
        import { GLTFLoader } from 'https://cdn.jsdelivr.net/npm/three@0.169.0/examples/jsm/loaders/GLTFLoader.js';
        import { DRACOLoader } from 'https://cdn.jsdelivr.net/npm/three@0.169.0/examples/jsm/loaders/DRACOLoader.js';
      
    </script> -->

    <script src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js" type="text/javascript"></script>
    <script type="text/javascript">
        WebFont.load({
            google: {
                families: ["Lato:100,100italic,300,300italic,400,400italic,700,700italic,900,900italic", "Montserrat:100,100italic,200,200italic,300,300italic,400,400italic,500,500italic,600,600italic,700,700italic,800,800italic,900,900italic", "Ubuntu:300,300italic,400,400italic,500,500italic,700,700italic", "Changa One:400,400italic", "Open Sans:300,300italic,400,400italic,600,600italic,700,700italic,800,800italic", "Varela Round:400", "Bungee Shade:regular", "Roboto:300,regular,500"]
            }
        });
    </script>
    <script type="text/javascript">
        ! function (o, c) {
            var n = c.documentElement,
                t = " w-mod-";
            n.className += t + "js", ("ontouchstart" in o || o.DocumentTouch && c instanceof DocumentTouch) && (n.className += t + "touch")
        }(window, document);
    </script>
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <script type="text/javascript" src="static/js/zoom.js"></script>
    <script type="text/javascript" src="static/js/video_comparison.js"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    </script>
</head>

<body>
    <!-- <div id="threejs-container" style="width: 100%; height: 500px;"></div> -->
    <div class="section hero nerf-_v2">
        <div class="container-2 nerf_header_v2 w-container">
            <h1 class="nerf_title_v2">SparseFlex: High-Resolution and<br /> Arbitrary-Topology 3D Shape Modeling (TripoSF)</h1>
            <!-- <div class="nerf_subheader_v2"></div> -->
            <div class="nerf_subheader_v2">
                <div>
                    <a href="https://xianglonghe.github.io/" target="_blank" class="nerf_authors_v2">Xianglong He*<span
                            class="text-span_nerf"></span></a><sup> 1,2</sup>,&nbsp;&nbsp;
                    <a href="https://scholar.google.com/citations?user=5TEI1eAAAAAJ" target="_blank" class="nerf_authors_v2">Zi-Xin Zou*<span
                            class="text-span_nerf"></span></a><sup> 2</sup>,&nbsp;&nbsp;
                    <a href="https://openreview.net/profile?id=~Chia_Hao_Chen1" target="_blank" class="nerf_authors_v2">Chia-Hao Chen<span
                        class="text-span_nerf"></span></a><sup> 1,2</sup>,&nbsp;&nbsp;
                    <a href="https://scholar.google.com/citations?user=b7ZJV9oAAAAJ" target="_blank" class="nerf_authors_v2">Yuan-Chen Guo<span
                            class="text-span_nerf"></span></a><sup> 2</sup>,&nbsp;&nbsp;
                    <a href="https://scholar.google.com/citations?user=Dqjnn0gAAAAJ" target="_blank" class="nerf_authors_v2">Ding Liang<span
                            class="text-span_nerf"></span></a><sup> 2</sup>,<br />
                    <a href="https://scholar.google.com/citations?user=fYdxi2sAAAAJ" target="_blank" class="nerf_authors_v2">Chun Yuan†<span
                            class="text-span_nerf"></span></a><sup> 1</sup>,&nbsp;&nbsp;
                    <a href="https://wlouyang.github.io/" target="_blank" class="nerf_authors_v2">Wanli Ouyang<span
                      class="text-span_nerf"></span></a><sup> 3</sup>,&nbsp;&nbsp;
                    <a href="https://yanpei.me/" target="_blank" class="nerf_authors_v2">Yan-Pei Cao<span
                      class="text-span_nerf"></span></a><sup> 2</sup>,&nbsp;&nbsp;
                    <a href="https://scholar.google.com/citations?user=a7AMvgkAAAAJ" target="_blank" class="nerf_authors_v2">Yangguang Li†<span
                      class="text-span_nerf"></span></a><sup> 2</sup>
                </div>
                <div>
                    <h1 class="nerf_affiliation_v2"><sup>1 </sup>Tsinghua University</h1>,&nbsp;&nbsp;
                    <h1 class="nerf_affiliation_v2"><sup>2 </sup>VAST</h1>,&nbsp;&nbsp;
                    <h1 class="nerf_affiliation_v2"><sup>3 </sup>The Chinese University of Hong Kong</h1>
                    <br>
                    <h1 class="nerf_affiliation_v2"><sup>* </sup>Equal Contributions</h1>,&nbsp;&nbsp;
                    <h1 class="nerf_affiliation_v2"><sup>† </sup>Corresponding Authors</h1>
                </div>

                <div class="external-link">
                  <a class="btn" href="https://arxiv.org/" role="button" target="_blank">
                      <i class="ai ai-arxiv"></i> Arxiv </a>
                  <a class="btn" href="https://github.com/VAST-AI-Research/TripoSF" role="button" target="_blank" disabled>
                      <i class="fa-brands fa-github"></i> Code </a>
                  <a class="btn" href="https://huggingface.co/VAST-AI/TripoSF" role="button" target="_blank" disabled>
                    <i class="fa-solid fa-chess-knight"></i> Model </a>
                </div>
                
            </div>
        </div>
    </div>
    <div class="white_section_nerf w-container" style="display: flex; flex-direction: column; align-items: center;">
        <div class="grid-container-1">
            <img src="assets/images/teaser.png">    
        </div>
    </div>

    <div data-anchor="slide1" class="section nerf_section">
        <div class="w-container grey_container" style="display: flex; flex-direction: column; align-items: center;">
            <h2 class="grey-heading_nerf" style="text-align: center;">Abstract</h2>
            <p class="paragraph-3 nerf_text nerf_results_text">
                  Creating high-fidelity 3D meshes with arbitrary topology,
                  including open surfaces and complex interiors, remains a significant challenge.
                  Existing implicit field methods often require costly and detail-degrading watertight conversion,
                  while other approaches struggle with high resolutions. This paper introduces <strong>SparseFlex</strong>,
                  a novel sparse-structured isosurface representation that enables differentiable mesh
                  reconstruction at resolutions up to 1024<sup>3</sup> directly from rendering losses.
                  SparseFlex combines the accuracy of Flexicubes with a sparse voxel structure,
                  focusing computation on surface-adjacent regions and efficiently handling open surfaces. 
                  Crucially, we introduce a frustum-aware sectional voxel training strategy that activates
                  only relevant voxels during rendering, dramatically reducing memory consumption and enabling
                  high-resolution training. This also allows, for the first time, the reconstruction of mesh
                  interiors using only rendering supervision. Building upon this, we demonstrate a complete
                  shape modeling pipeline by training a variational autoencoder (VAE) and a rectified
                  flow transformer for high-quality 3D shape generation. Our experiments show state-of-the-art
                  reconstruction accuracy, with a ~82% reduction in Chamfer Distance and a ~88% increase 
                  in F-score compared to previous methods, and demonstrate the generation of high-resolution,
                  detailed 3D shapes with arbitrary topology. By enabling high-resolution, differentiable
                  mesh reconstruction and generation with rendering losses, SparseFlex significantly
                  advances the state-of-the-art in 3D shape representation and modeling.
                <br>
                <!-- <img  src="assets/images/overview.png"> -->
            </p>
        </div>
    </div>

    <!-- <script type="module">
        import * as THREE from 'https://cdn.jsdelivr.net/npm/three@0.169.0/build/three.module.js';
        import { OrbitControls } from 'https://cdn.jsdelivr.net/npm/three@0.169.0/examples/jsm/controls/OrbitControls.js';
        import { GLTFLoader } from 'https://cdn.jsdelivr.net/npm/three@0.169.0/examples/jsm/loaders/GLTFLoader.js';
        import { DRACOLoader } from 'https://cdn.jsdelivr.net/npm/three@0.169.0/examples/jsm/loaders/DRACOLoader.js';

        // 初始化场景
        const scene = new THREE.Scene();
        scene.background = new THREE.Color(0x333333);
        const camera = new THREE.PerspectiveCamera(75, window.innerWidth / window.innerHeight, 0.1, 1000);
        camera.position.set(2, 2, 5);

        // 初始化渲染器并添加到容器
        const renderer = new THREE.WebGLRenderer({ antialias: true });
        renderer.setSize(window.innerWidth, 500); // 高度设为500px
        document.getElementById('threejs-container').appendChild(renderer.domElement);

        // 控制器
        const controls = new OrbitControls(camera, renderer.domElement);
        controls.enableDamping = true;

        // 光源
        const light = new THREE.DirectionalLight(0xffffff, 1);
        light.position.set(1, 1, 1);
        scene.add(light, new THREE.AmbientLight(0x404040));

        // 加载模型
        const dracoLoader = new DRACOLoader();
        dracoLoader.setDecoderPath('https://cdn.jsdelivr.net/npm/three@0.169.0/examples/js/libs/draco/gltf/'); // 本地路径

        const loader = new GLTFLoader();
        loader.setDRACOLoader(dracoLoader);
        loader.load(
            'assets/samples/gt/flower.drc', // 确认路径正确
            (gltf) => {
                const model = gltf.scene;
                scene.add(model);
                model.position.set(0, 0, 0);
            },
            undefined,
            (error) => console.error('加载失败:', error)
        );

        // 动画循环
        function animate() {
            requestAnimationFrame(animate);
            controls.update();
            renderer.render(scene, camera);
        }
        animate();

        // 响应式调整
        window.addEventListener('resize', () => {
            camera.aspect = window.innerWidth / window.innerHeight;
            camera.updateProjectionMatrix();
            renderer.setSize(window.innerWidth, 500);
        });
    </script> -->

    <div class="white_section_nerf w-container" style="display: flex; flex-direction: column; align-items: center;">
        <h2 class="grey-heading_nerf" style="text-align: center;">VAE Reconstruction Results</h2>
        <div class="grid-container-4">
            <div>
                <p class="myprompt nerf_text"> GT Mesh </p>
                <model-viewer exposure="0.5" camera-controls enable-pan shadow-intensity="2" camera-orbit="0deg 75deg 150%" max-camera-orbit="auto auto 100%"
                src="assets/reconstruction/monster_textured.glb">
            </div>
            <div>
                <p class="myprompt nerf_text"> Reconstructed Mesh </p>
                <model-viewer exposure="0.5" camera-controls enable-pan shadow-intensity="2" camera-orbit="0deg 75deg 150%" max-camera-orbit="auto auto 100%"
                src="assets/reconstruction/monster_textured.glb">
                </model-viewer>
            </div>
            <div>
                <p class="myprompt nerf_text"> GT Mesh </p>
                <model-viewer exposure="0.5" camera-controls enable-pan shadow-intensity="2" camera-orbit="0deg 75deg 150%" max-camera-orbit="auto auto 100%"
                src="assets/reconstruction/monster_textured.glb">
            </div>
            <div>
                <p class="myprompt nerf_text"> Reconstructed Mesh </p>
                <model-viewer exposure="0.5" camera-controls enable-pan shadow-intensity="2" camera-orbit="0deg 75deg 150%" max-camera-orbit="auto auto 100%"
                src="assets/reconstruction/monster_textured.glb">
                </model-viewer>
            </div>
        </div>
        <div class="grid-container-4">
            <div>
                <p class="myprompt nerf_text"> GT Mesh </p>
                <model-viewer exposure="0.5" camera-controls enable-pan shadow-intensity="2" camera-orbit="0deg 75deg 150%" max-camera-orbit="auto auto 100%"
                src="assets/reconstruction/monster_textured.glb">
            </div>
            <div>
                <p class="myprompt nerf_text"> Reconstructed Mesh </p>
                <model-viewer exposure="0.5" camera-controls enable-pan shadow-intensity="2" camera-orbit="0deg 75deg 150%" max-camera-orbit="auto auto 100%"
                src="assets/reconstruction/monster_textured.glb">
                </model-viewer>
            </div>
            <div>
                <p class="myprompt nerf_text"> GT Mesh </p>
                <model-viewer exposure="0.5" camera-controls enable-pan shadow-intensity="2" camera-orbit="0deg 75deg 150%" max-camera-orbit="auto auto 100%"
                src="assets/reconstruction/monster_textured.glb">
            </div>
            <div>
                <p class="myprompt nerf_text"> Reconstructed Mesh </p>
                <model-viewer exposure="0.5" camera-controls enable-pan shadow-intensity="2" camera-orbit="0deg 75deg 150%" max-camera-orbit="auto auto 100%"
                src="assets/reconstruction/monster_textured.glb">
                </model-viewer>
            </div>
        </div>
    </div>


    <div class="white_section_nerf w-container" style="display: flex; flex-direction: column; align-items: center;">
        <h2 class="grey-heading_nerf" style="text-align: center;">Image-to-3D Generation Results (Video)</h2>
        <p class="myprompt nerf_text"> Some images are from 
            <a href="https://www.tripo3d.ai/" target="_blank" class="nerf_authors_v2">Tripo<span
                class="text-span_nerf"></span></a>,
            <a href="https://hyperhuman.deemos.com/rodin" target="_blank" class="nerf_authors_v2">Rodin<span
                class="text-span_nerf"></span></a>,
            <a href="https://www.meshy.ai/" target="_blank" class="nerf_authors_v2">Meshy<span
                class="text-span_nerf"></span></a>,
                <a href="https://trellis3d.github.io/" target="_blank" class="nerf_authors_v2">Trellis<span
                    class="text-span_nerf"></span></a>.
            </p>
        <div class="grid-container-4">
            <img src="assets/images/image_to_3d/anime_rocker_bassist_leather_jacket.png" width="210" height="210">
            <div>
                <video class="video" loop playsinline autoPlay muted 
                src="assets/videos/image_to_3d/lfbUueGj.mp4" onplay="resizeAndPlay(this)"></video>
            </div>
            <img src="assets/images/image_to_3d/blue_alien_head_tech_base.png" width="230" height="230">
            <div>
                <video class="video" loop playsinline autoPlay muted 
                src="assets/videos/image_to_3d/rNvBTgqF.mp4" onplay="resizeAndPlay(this)"></video>
            </div>
            <img src="assets/images/image_to_3d/demon_face_carved_pumpkin.png" width="210" height="210">
            <div>
                <video class="video" loop playsinline autoPlay muted 
                src="assets/videos/image_to_3d/VPnI1DhF.mp4" onplay="resizeAndPlay(this)"></video>
            </div>
            <img src="assets/images/image_to_3d/energetic_boy_running_colorblock_jacket.png" width="230" height="230">
            <div>
                <video class="video" loop playsinline autoPlay muted 
                src="assets/videos/image_to_3d/eBX7z9BG.mp4" onplay="resizeAndPlay(this)"></video>
            </div>
            <img src="assets/images/image_to_3d/insectoid_robot_walker_with_turquoise_viewport.png" width="210" height="210">
            <div>
                <video class="video" loop playsinline autoPlay muted 
                src="assets/videos/image_to_3d/o0LxoHqU.mp4" onplay="resizeAndPlay(this)"></video>
            </div>
            <img src="assets/images/image_to_3d/typical_misc_workbench.png" width="230" height="230">
            <div>
                <video class="video" loop playsinline autoPlay muted 
                src="assets/videos/image_to_3d/T2ZOeHhg.mp4" onplay="resizeAndPlay(this)"></video>
            </div>
        </div>
    </div>

    <div class="white_section_nerf w-container" style="display: flex; flex-direction: column; align-items: center;">
        <h2 class="grey-heading_nerf" style="text-align: center;">Method Overview</h2>
        <div class="grid-container-1">
            <img src="assets/images/pipeline.png" loading="lazy" alt="Pipeline">

            <p> Our TripoSF VAE takes point clouds sampled from a mesh as input, voxelizes them, and aggregates their features into each voxel.
                A sparse transformer encoder-decoder compresses the structured feature into a more compact latent space,
                followed by a self-pruning upsampling for higher resolution. Finally, the structured features are decoded
                to SparseFlex representation through a linear layer. Using the frustum-aware section voxel training strategy,
                we can train the entire pipeline more efficiently by rendering loss.
            </p>
        </div>
    </div>

    <div class="white_section_nerf w-container" style="display: flex; flex-direction: column; align-items: center;">
        <h2 class="grey-heading_nerf" style="text-align: center;">Frustum-aware Sectional Voxel Strategy</h2>
        <div class="grid-container-1">
            <img src="assets/images/strategy.png" style="display: block; margin: 0 auto; width: 80%; max-width: 600px; height: auto;" loading="lazy" alt="Strategy">

            <p> The previous mesh-based rendering training strategy (left) requires activating the entire dense grid to extract
                the mesh surface, even though only a few voxels are necessary during rendering. In contrast, our frustum-aware section voxel training strategy (right)
                adaptively activates the relevant voxels and enables the reconstruction of mesh interiors only using rendering supervision.
            </p>
        </div>
    </div>
    
    <!-- <div class="white_section_nerf  w-container">
        <h2 class="grey-heading_nerf">Related Works</h2>
        <div class="grid-container-1">
	    <p> 
		    <a href="https://neuralcarver.github.io/michelangelo/" target="_blank">Michelangelo</span></a>: 
a Shape-Image-Text-Aligned 3D Variational Auto-Encoder (SITA-VAE) and a conditional Aligned 3D Shape Latent Diffusion Model (ASLDM). Thanks for their open source;<br>
	    	    <a href="https://github.com/CLAY-3D/OpenCLAY" target="_blank">OpenCLAY</a>:
a large-scale generative model composed of a multi-resolution 3D Variational Autoencoder (VAE) and a minimalistic latent 3D Diffusion Transformer (DiT);<br>
		    <a href="https://nju-3dv.github.io/projects/Direct3D/" target="_blank">Direct3D</a>:
a Direct 3D Variational Auto-Encoder (D3D-VAE) and a Direct 3D Diffusion Transformer (D3D-DiT) scalable to in-the-wild input single view images;
	    </p>
        </div>
    </div> -->

<!-- <div class="white_section_nerf grey_container w-container">
<h2 class="grey-heading_nerf">BibTeX</h2>
<div class="bibtex">
    <pre><code>@article{li2024craftsman,
author    = {Weiyu Li and Jiarui Liu and Hongyu Yan and Rui Chen and Yixun Liang and Xuelin Chen and Ping Tan and Xiaoxiao Long},
title     = {CraftsMan3D: High-fidelity Mesh Generation with 3D Native Generation and Interactive Geometry Refiner},
journal   = {arXiv preprint arXiv:2405.14979},
year      = {2024},
}</code></pre>
</div>
</div> -->

</body>
<footer>
    This project page is inspired by <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
</footer>

</html>