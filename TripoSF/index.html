<!DOCTYPE html>
<head>
    <meta charset="utf-8" />
    <title>SparseFlex: High-Resolution and Arbitrary-Topology 3D Shape Modeling</title>
	<link rel="icon" type="image/x-icon" href="assets/images/favicon.ico">
    <meta content="SparseFlex: High-Resolution and Arbitrary-Topology 3D Shape Modeling" name="description" />
    <meta content="summary" name="twitter:card" />
    <meta content="width=device-width, initial-scale=1" name="viewport" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
    <script type="module" src="https://unpkg.com/@google/model-viewer/dist/model-viewer.min.js"></script>
    <link href="static/css/template.css" rel="stylesheet" type="text/css" />
    <link href="static/css/my_style.css" rel="stylesheet" type="text/css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-WW8QDKVM84"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-WW8QDKVM84');
    </script>

    <script src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js" type="text/javascript"></script>
    <script type="text/javascript">
        WebFont.load({
            google: {
                families: ["Lato:100,100italic,300,300italic,400,400italic,700,700italic,900,900italic", "Montserrat:100,100italic,200,200italic,300,300italic,400,400italic,500,500italic,600,600italic,700,700italic,800,800italic,900,900italic", "Ubuntu:300,300italic,400,400italic,500,500italic,700,700italic", "Changa One:400,400italic", "Open Sans:300,300italic,400,400italic,600,600italic,700,700italic,800,800italic", "Varela Round:400", "Bungee Shade:regular", "Roboto:300,regular,500"]
            }
        });
    </script>
    <script type="text/javascript">
        ! function (o, c) {
            var n = c.documentElement,
                t = " w-mod-";
            n.className += t + "js", ("ontouchstart" in o || o.DocumentTouch && c instanceof DocumentTouch) && (n.className += t + "touch")
        }(window, document);
    </script>
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <script type="text/javascript" src="static/js/zoom.js"></script>
    <script type="text/javascript" src="static/js/video_comparison.js"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    </script>

<!--    </script>-->
<style type="text/css">
.w3-col.l2 {
    width: 20%;
}

@media (max-width: 1200px) {
    .w3-col.m4 {
        width: 33.33333%;
    }
}

@media (max-width: 600px) {
    .w3-col.m4 {
        width: 50%;
    }
}

.vidContainer {
    padding: 0 8px !important; /* 减小内边距 */
}

    .meshViewer {
        height: 200px;
        width: 200px;
        border: none;
        overflow: hidden;
        display: none;
    }

    .video {
        width: 200px;
        height: 200px;
        position: relative;
        top: 0;
        left: 0;
    }

    .videoMerge {
        position: relative;
        top: 0;
        left: 0;
        z-index: 10;
        width: 100%;
        display: block;
        margin: 0 auto;
        background-size: cover;
    }

    .viewerToggle {
        display: block;
        text-align: center;
        overflow: hidden;
        border: none;
        margin-top: 10px;
    }
    

    .viewerToggle:hover {
        text-decoration: none;
    }

    #load_more {
        text-decoration: none;
    }

    .largeContainer {
        display: flex;
        flex-wrap: wrap;
        align-items: flex-end;
        row-gap: 30px;
    }

    .emptydiv {
        height: 59px;
    }

    .div-only-desk {
    }

    .div-only-mobile {
    }

    @media screen and (min-width: 1240px) and (-webkit-min-device-pixel-ratio: 0) and (min-resolution: 0.001dpcm) {
        .div-only-desk {
            visibility: visible;
        }

        .div-only-mobile {
            visibility: hidden;
            display: none;
        }
    }

    @media screen and (max-width: 1239px) {
        .div-only-desk {
            visibility: hidden;
            display: none;
        }

        .div-only-mobile {
            visibility: visible;
        }
    }

    @keyframes placeHolderShimmer {
        0% {
            background-position: -800px 0
        }
        100% {
            background-position: 800px 0
        }
    }

    .animated-background {
        animation-duration: 2s;
        animation-fill-mode: forwards;
        animation-iteration-count: infinite;
        animation-name: placeHolderShimmer;
        animation-timing-function: linear;
        background-color: #f6f7f8;
        background: linear-gradient(to right, #eeeeee 8%, #bbbbbb 18%, #eeeeee 33%);
        background-size: 800px 104px;
        height: 256px;
        width: 256px;
        position: relative;
    }

</style>
</head>

<body>
    <!-- <div id="threejs-container" style="width: 100%; height: 500px;"></div> -->
    <div class="section hero nerf-_v2">
        <div class="container-2 nerf_header_v2 w-container">
            <h1 class="nerf_title_v2">SparseFlex: High-Resolution and<br /> Arbitrary-Topology 3D Shape Modeling (<span style="color: red;">TripoSF</span>)</h1>
            <!-- <div class="nerf_subheader_v2"></div> -->
            <div class="nerf_subheader_v2">
                <div>
                    <a href="https://xianglonghe.github.io/" target="_blank" class="nerf_authors_v2">Xianglong He*<span
                            class="text-span_nerf"></span></a><sup> 1,2</sup>,&nbsp;&nbsp;
                    <a href="https://scholar.google.com/citations?user=5TEI1eAAAAAJ" target="_blank" class="nerf_authors_v2">Zi-Xin Zou*<span
                            class="text-span_nerf"></span></a><sup> 2</sup>,&nbsp;&nbsp;
                    <a href="https://openreview.net/profile?id=~Chia_Hao_Chen1" target="_blank" class="nerf_authors_v2">Chia-Hao Chen<span
                        class="text-span_nerf"></span></a><sup> 1,2</sup>,&nbsp;&nbsp;
                    <a href="https://scholar.google.com/citations?user=b7ZJV9oAAAAJ" target="_blank" class="nerf_authors_v2">Yuan-Chen Guo<span
                            class="text-span_nerf"></span></a><sup> 2</sup>,&nbsp;&nbsp;
                    <a href="https://scholar.google.com/citations?user=Dqjnn0gAAAAJ" target="_blank" class="nerf_authors_v2">Ding Liang<span
                            class="text-span_nerf"></span></a><sup> 2</sup>,<br />
                    <a href="https://scholar.google.com/citations?user=fYdxi2sAAAAJ" target="_blank" class="nerf_authors_v2">Chun Yuan<sup>†</sup><span
                            class="text-span_nerf"></span></a><sup> 1</sup>,&nbsp;&nbsp;
                    <a href="https://wlouyang.github.io/" target="_blank" class="nerf_authors_v2">Wanli Ouyang<span
                      class="text-span_nerf"></span></a><sup> 3</sup>,&nbsp;&nbsp;
                    <a href="https://yanpei.me/" target="_blank" class="nerf_authors_v2">Yan-Pei Cao<span
                      class="text-span_nerf"></span></a><sup> 2</sup>,&nbsp;&nbsp;
                    <a href="https://scholar.google.com/citations?user=a7AMvgkAAAAJ" target="_blank" class="nerf_authors_v2">Yangguang Li<sup>†</sup><span
                      class="text-span_nerf"></span></a><sup> 2</sup>
                </div>
                <div>
                    <h1 class="nerf_affiliation_v2"><sup>1 </sup>Tsinghua University</h1>,&nbsp;&nbsp;
                    <h1 class="nerf_affiliation_v2"><sup>2 </sup>VAST</h1>,&nbsp;&nbsp;
                    <h1 class="nerf_affiliation_v2"><sup>3 </sup>The Chinese University of Hong Kong</h1>
                    <br>
                    <h1 class="nerf_affiliation_v2"><sup>* </sup>Equal Contributions</h1>,&nbsp;&nbsp;
                    <h1 class="nerf_affiliation_v2"><sup>† </sup>Corresponding Authors</h1>
                </div>

                <div class="external-link">
                  <a class="btn" href="https://arxiv.org/" role="button" target="_blank">
                      <i class="ai ai-arxiv"></i> Arxiv </a>
                  <a class="btn" href="assets/triposf.pdf" role="button" target="_blank">
                      <i class="fa fa-file-pdf"></i> Paper </a>
                  <a class="btn" href="https://github.com/VAST-AI-Research/TripoSF" role="button" target="_blank" disabled>
                      <i class="fa-brands fa-github"></i> Code </a>
                  <a class="btn" href="https://huggingface.co/VAST-AI/TripoSF" role="button" target="_blank" disabled>
                    <i class="fa-solid fa-chess-knight"></i> Model </a>
                </div>
                
            </div>
        </div>
    </div>
    <div class="white_section_nerf w-container" style="display: flex; flex-direction: column; align-items: center;">
        <div class="grid-container-1">
            <img src="assets/images/teaser.png">    
        </div>
    </div>

    <div data-anchor="slide1" class="section nerf_section">
        <div class="w-container grey_container" style="display: flex; flex-direction: column; align-items: center;">
            <h2 class="grey-heading_nerf" style="text-align: center;">Abstract</h2>
            <p class="paragraph-3 nerf_text nerf_results_text">
                  Creating high-fidelity 3D meshes with arbitrary topology,
                  including open surfaces and complex interiors, remains a significant challenge.
                  Existing implicit field methods often require costly and detail-degrading watertight conversion,
                  while other approaches struggle with high resolutions. This paper introduces <strong>SparseFlex</strong>,
                  a novel sparse-structured isosurface representation that enables differentiable mesh
                  reconstruction at resolutions up to 1024<sup>3</sup> directly from rendering losses.
                  SparseFlex combines the accuracy of Flexicubes with a sparse voxel structure,
                  focusing computation on surface-adjacent regions and efficiently handling open surfaces. 
                  Crucially, we introduce a frustum-aware sectional voxel training strategy that activates
                  only relevant voxels during rendering, dramatically reducing memory consumption and enabling
                  high-resolution training. This also allows, for the first time, the reconstruction of mesh
                  interiors using only rendering supervision. Building upon this, we demonstrate a complete
                  shape modeling pipeline by training a variational autoencoder (VAE) and a rectified
                  flow transformer for high-quality 3D shape generation. Our experiments show state-of-the-art
                  reconstruction accuracy, with a ~82% reduction in Chamfer Distance and a ~88% increase 
                  in F-score compared to previous methods, and demonstrate the generation of high-resolution,
                  detailed 3D shapes with arbitrary topology. By enabling high-resolution, differentiable
                  mesh reconstruction and generation with rendering losses, SparseFlex significantly
                  advances the state-of-the-art in 3D shape representation and modeling.
                <br>
                <!-- <img  src="assets/images/overview.png"> -->
            </p>
        </div>
    </div>


    <!-- ReconSection -->
    <div class="w3-content w3-padding" style="max-width:1200px">
        <!-- ReconSection -->
        <h2 class="grey-heading_nerf" style="text-align: center;">VAE Reconstruction Results</h2>
        <div id='textToVideos' class="w3-row-padding largeContainer">
        </div>
        <a href="#"  id='load_more' onclick="add_videos(); return false;" class="w3-button w3-pale-red w3-block" style="margin-top: 15px;">Load more samples</a>
        <!-- End page content -->
    </div>
    

<script type="text/javascript">
    vid_button = document.getElementsByClassName("mesh-button");
    for (var i = 0; i < vid_button.length; i++) {
        let id = vid_button[i].id;
        let id_base = id.substring(0, id.length - 7);
        let button = document.getElementById(id);
        let canvas = document.getElementById(id_base + "Merge");
        let model = document.getElementById(id_base + "-model");
        button.onclick = function () {
            canvas.style.display = "none";
            button.style.display = "none";
            model.style.display = "block";
        }
    }

</script>
<script src="static/js/scripts.js"></script>

    <div class="white_section_nerf w-container" style="display: flex; flex-direction: column; align-items: center;">
        <h2 class="grey-heading_nerf" style="text-align: center;">Image-to-3D Generation Results (Video)</h2>
        <p class="myprompt nerf_text"> Some images are from 
            <a href="https://www.tripo3d.ai/" target="_blank" class="nerf_authors_v2">Tripo<span
                class="text-span_nerf"></span></a>,
            <a href="https://trellis3d.github.io/" target="_blank" class="nerf_authors_v2">Trellis<span
                class="text-span_nerf"></span></a>,
            Rodin,
            Meshy.
            </p>
        <div class="grid-container-4">
            <img src="assets/images/image_to_3d/a1.jpg" width="200" height="200">
            <div>
                <video class="video" loop playsinline autoPlay muted 
                src="assets/videos/image_to_3d/a3.glb.mp4" onplay="resizeAndPlay2(this)"></video>
            </div>
            <img src="assets/images/image_to_3d/a2.jpg" width="200" height="200">
            <div>
                <video class="video" loop playsinline autoPlay muted 
                src="assets/videos/image_to_3d/a4.glb.mp4" onplay="resizeAndPlay2(this)"></video>
            </div>
            <img src="assets/images/image_to_3d/a3.jpg" width="200" height="200">
            <div>
                <video class="video" loop playsinline autoPlay muted 
                src="assets/videos/image_to_3d/a2.mp4" onplay="resizeAndPlay2(this)"></video>
            </div>
            <img src="assets/images/image_to_3d/a4.jpg" width="200" height="200">
            <div>
                <video class="video" loop playsinline autoPlay muted 
                src="assets/videos/image_to_3d/a7.glb.mp4" onplay="resizeAndPlay2(this)"></video>
            </div>
            <img src="assets/images/image_to_3d/a5.png" width="200" height="200">
            <div>
                <video class="video" loop playsinline autoPlay muted 
                src="assets/videos/image_to_3d/a1.mp4" onplay="resizeAndPlay2(this)"></video>
            </div>
            <img src="assets/images/image_to_3d/a6.jpg" width="200" height="200">
            <div>
                <video class="video" loop playsinline autoPlay muted 
                src="assets/videos/image_to_3d/a6.mp4" onplay="resizeAndPlay2(this)"></video>
            </div>
            <img src="assets/images/image_to_3d/a7.png" width="200" height="200">
            <div>
                <video class="video" loop playsinline autoPlay muted 
                src="assets/videos/image_to_3d/a5.glb.mp4" onplay="resizeAndPlay2(this)"></video>
            </div>
            <img src="assets/images/image_to_3d/a8.jpg" width="200" height="200">
            <div>
                <video class="video" loop playsinline autoPlay muted 
                src="assets/videos/image_to_3d/a9.glb.mp4" onplay="resizeAndPlay2(this)"></video>
            </div>
        </div>
    </div>

    <div class="white_section_nerf w-container" style="display: flex; flex-direction: column; align-items: center;">
        <h2 class="grey-heading_nerf" style="text-align: center;">Method Overview</h2>
        <div class="grid-container-1">
            <img src="assets/images/pipeline.png" loading="lazy" alt="Pipeline">

            <p> Our TripoSF VAE takes point clouds sampled from a mesh as input, voxelizes them, and aggregates their features into each voxel.
                A sparse transformer encoder-decoder compresses the structured feature into a more compact latent space,
                followed by a self-pruning upsampling for higher resolution. Finally, the structured features are decoded
                to SparseFlex representation through a linear layer. Using the frustum-aware section voxel training strategy,
                we can train the entire pipeline more efficiently by rendering loss.
            </p>
        </div>
    </div>

    <div class="white_section_nerf w-container" style="display: flex; flex-direction: column; align-items: center;">
        <h2 class="grey-heading_nerf" style="text-align: center;">Frustum-aware Sectional Voxel Strategy</h2>
        <div class="grid-container-1">
            <img src="assets/images/strategy.png" style="display: block; margin: 0 auto; width: 80%; max-width: 600px; height: auto;" loading="lazy" alt="Strategy">

            <p> The previous mesh-based rendering training strategy (left) requires activating the entire dense grid to extract
                the mesh surface, even though only a few voxels are necessary during rendering. In contrast, our frustum-aware section voxel training strategy (right)
                adaptively activates the relevant voxels and enables the reconstruction of mesh interiors only using rendering supervision.
            </p>
        </div>
    </div>
    

</body>
<footer>
    This project page is inspired by <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> and <a href="https://make-a-video3d.github.io">Make-A-Video3D</a>.
</footer>

</html>